# tests/test_api.py
import os
import time
import json
import pytest
from pathlib import Path
from fastapi.testclient import TestClient

# The `client` and `test_data_dir` fixtures are provided by tests/conftest.py
# `patch_store_env` in conftest ensures STORE_DIR env var points to a temp dir.

def test_healthz_and_docs_openapi(client: TestClient):
    # health
    r = client.get("/healthz")
    assert r.status_code == 200
    assert r.json().get("status") == "ok"

    # openapi and docs reachable
    r = client.get("/openapi.json")
    assert r.status_code == 200
    assert "Contract Intelligence API" in r.json().get("info", {}).get("title", "Contract Intelligence API")

    r = client.get("/docs")
    assert r.status_code in (200, 304)  # 304 if cached


def test_metrics_endpoint(client: TestClient):
    r = client.get("/metrics")
    assert r.status_code == 200
    text = r.content.decode("utf-8")
    # Should contain at least one prometheus metric line
    assert "process_cpu_seconds_total" in text or "python_info" in text


@pytest.mark.skipif(not Path("data/nda.pdf").exists(), reason="sample NDA PDF missing in data/")
def test_ingest_extract_audit_and_qa_flow(client: TestClient):
    """
    End-to-end: ingest sample PDFs, wait for background processing, then call extract, audit and ask.
    """
    data_dir = Path("data")
    nda_path = data_dir / "nda.pdf"
    msa_path = data_dir / "msa.pdf"

    files = []
    # attach files that exist
    if nda_path.exists():
        files.append(("files", ("nda.pdf", open(nda_path, "rb"), "application/pdf")))
    if msa_path.exists():
        files.append(("files", ("msa.pdf", open(msa_path, "rb"), "application/pdf")))

    assert files, "No sample pdfs found in data/ to upload"

    # Ingest
    r = client.post("/ingest", files=files)
    for _, fh in files:
        # make sure to close files we've opened
        try:
            fh[1].close()
        except Exception:
            pass

    assert r.status_code in (200, 201)
    body = r.json()
    assert "document_ids" in body
    doc_ids = body["document_ids"]
    assert isinstance(doc_ids, list)
    assert len(doc_ids) >= 1

    # After ingest, background tasks should extract text and save files under STORE_DIR.
    # Patch_store_env (conftest) set STORE_DIR to a temporary path; read env to find it.
    store_dir = Path(os.environ.get("STORE_DIR", "store"))
    pdfs_dir = store_dir / "pdfs"
    texts_dir = store_dir / "texts"
    index_file = store_dir / "index.json"

    # Wait/poll for files and saved text to appear (background processing).
    deadline = time.time() + 10.0
    found_text = False
    while time.time() < deadline:
        # check pdf files present
        pdf_exists = all((pdfs_dir / f"{did}.pdf").exists() for did in doc_ids)
        # check text files: background extraction may create texts
        text_exists = any((texts_dir / f"{did}.txt").exists() for did in doc_ids)
        if pdf_exists and text_exists:
            found_text = True
            break
        time.sleep(0.5)

    assert pdf_exists, f"Expected uploaded PDFs to be present under {pdfs_dir}"
    # text may not be present if extraction failed; make it non-fatal but warn
    assert found_text, f"No extracted text files appear under {texts_dir} within 10s; background extraction may have failed"

    # pick first doc_id for follow-ups
    document_id = doc_ids[0]

    # Extract structured fields
    r = client.post("/extract", data={"document_id": document_id})
    # Extraction may fail if the text extraction wasn't finished; allow 200 or 500 but prefer 200
    assert r.status_code in (200, 500, 404)
    if r.status_code == 200:
        payload = r.json()
        # ensure basic fields exist in response (as described in assignment)
        assert isinstance(payload, dict)
        # fields may be present under payload["fields"] in some implementations, but our extractor returns full dict
        # check for at least metadata keys or fields keys
        have_some = any(k in payload for k in ("fields", "raw_text", "document_id")) or ("governing_law" in payload)
        assert have_some

    # Audit endpoint
    r = client.post("/audit", data={"document_id": document_id})
    assert r.status_code in (200, 500, 404)
    if r.status_code == 200:
        audit_resp = r.json()
        assert "document_id" in audit_resp
        assert audit_resp["document_id"] == document_id

    # QA: ask a generic question - since pipeline may not have indexed fully, accept empty answer with score 0
    r = client.post("/ask", data={"question": "what is the governing law?"})
    assert r.status_code == 200
    qaresp = r.json()
    assert isinstance(qaresp, dict)
    assert "answer" in qaresp and "score" in qaresp and "citations" in qaresp

    # streaming endpoint (SSE fallback or EventSource should respond)
    rs = client.get("/ask/stream", params={"q": "governing law"}, stream=True)
    assert rs.status_code == 200
    # Try to read some data from streaming response (if available)
    try:
        # iterate a few chunks safely
        data = next(rs.iter_lines(chunk_size=128, timeout=1))
        # if streaming produced bytes, decode is fine (may be empty)
        assert data is not None
    except StopIteration:
        # streaming produced no chunks within timeout â€” acceptable fallback
        pass
    except TypeError:
        # older starlette/testclient may not support timeout param; try default iteration
        try:
            data = next(rs.iter_lines())
        except Exception:
            data = None
        # don't assert hard here
    except Exception:
        # Do not fail the entire test if streaming iteration has environment-specific behavior
        pass


@pytest.mark.skipif(not Path("data/nda.pdf").exists(), reason="sample NDA PDF missing in data/")
def test_ingest_returns_valid_document_ids_and_indexing(client: TestClient):
    """
    Sanity check that ingest returns ids that map to stored PDF files and index.json contains entries.
    """
    data_dir = Path("data")
    nda_path = data_dir / "nda.pdf"
    with open(nda_path, "rb") as fh:
        files = {"files": ("nda.pdf", fh, "application/pdf")}
        r = client.post("/ingest", files=files)

    assert r.status_code in (200, 201)
    body = r.json()
    doc_ids = body.get("document_ids", [])
    assert isinstance(doc_ids, list) and len(doc_ids) >= 1

    store_dir = Path(os.environ.get("STORE_DIR", "store"))
    index_file = store_dir / "index.json"

    # index.json should be present and contain our doc id (may be added quickly)
    deadline = time.time() + 6.0
    found = False
    while time.time() < deadline:
        if index_file.exists():
            try:
                idx = json.loads(index_file.read_text(encoding="utf-8"))
                docs = idx.get("documents", {})
                for did in doc_ids:
                    if did in docs:
                        found = True
                        break
            except Exception:
                pass
        if found:
            break
        time.sleep(0.5)

    assert found, f"Index file {index_file} did not register ingested doc ids within timeout"
